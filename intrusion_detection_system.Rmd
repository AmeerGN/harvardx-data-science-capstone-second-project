---
title: "Intrusion Detection Systems (IDSs) using NSL-KDD Dataset"
author: "Ameer Nasrallah"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    number_sections: true
    toc: true
    extra_dependencies: ["flafter"]
header-includes:
  - \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
bibliography: references.bib
csl: ieee.csl
---

\newpage

# Introduction

An intrusion detection system (IDS) is a system that automatically checks and analyzes network flow to detect and prevent abnormal activities [@chen2018effective]. This includes monitoring both user and system behaviors, such as the unauthorized access to network resources, and the analysis of network packet fields (e.g. IP address, flag, ports) [@selvakumar2018firefly]. Upon detecting an intrusion, the IDS alarms it to the management [@hajisalem2018hybrid].

An IDS can be classified based on the detection mechanism to knowledge-based [@chen2018effective] and behavior-based [@selvakumar2018firefly]. In knowledge-based (or signature-based) detection, the IDS uses misuse detection to detect known attacks by comparing between received packets and a predefined set of collected data (e.g. signature files). While in behavior-based (or anomaly-based) detection, the IDS uses anomaly detection to detect unknown attacks by comparing the system state with the normal activity profile it builds. Anomaly-based detection suffers from high false alarm rates. Despite that, it is still considered better than knowledge-based detection as it can detect novel or zero-day attacks. Hence, anomaly-based detection got more attention during the past twenty years, and there have been many research efforts to apply machine learning techniques in intrusion detection.

The goal of this project is to build two anomaly-based IDSs. The first system is a binary classification system that will classify a TCP connection (defined by certain attributes such as the port, protocol, etc) as either a normal activity or as an attack. While the second system will be a multi-class classification system that will classify the TCP connection as a normal activity or as one of four known attacks (DoS, Probing, R2L or U2R). Both systems will be built using NSL-KDD dataset [@tavallaee2009detailed] which provides the predictions of about 150000 simulated TCP connections.

We will evaluate the performance of each system using six classifiers (Recursive Partitioning, Naive Bayes, KNN, SVM, Random Forest, and Multi-Layer Perceptron), given that the data is already divided into training and testing sets (about 15% of the data). For each classifier, we will apply a tune grid search with 10-fold cross validation on the training set only to find the best tuned parameters. Then, the performance of each tuned classifier will be measured by predicting the testing set output. We will report different metrics, but we will focus mainly on three of them: overall accuracy (max), detection rates (max), and false alarm rates (min).

The rest of this report is organized as follows. We will start by studying the characteristics of NSL-KDD dataset. Then, we will explain the the methodology. Then, we will analyze the training and performance of the binary classification IDS. Then, we will analyze the training and performance of the multi-class classification IDS.. And finally, we will conclude all the work.

# NSL-KDD Dataset Characteristics

```{r Read NSL-KDD training and testing sets, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(recipes)
library(doParallel)
library(knitr)

##########################################################
# Create train set, test set (final hold-out test set)
##########################################################

# Define NSL-KDD column names
basic_features <- c("duration",
                    "protocol_type",
                    "service",
                    "flag",
                    "src_bytes",
                    "dst_bytes",
                    "land",
                    "wrong_fragment",
                    "urgent")

content_features <- c("hot",
                      "num_failed_logins",
                      "logged_in",
                      "num_compromised",
                      "root_shell",
                      "su_attempted",
                      "num_root",
                      "num_file_creations",
                      "num_shells",
                      "num_access_files",
                      "num_outbound_cmds",
                      "is_host_login",
                      "is_guest_login")

time_based_traffic_features <- c("count",
                                 "srv_count",
                                 "serror_rate",
                                 "srv_serror_rate",
                                 "rerror_rate",
                                 "srv_rerror_rate",
                                 "same_srv_rate",
                                 "diff_srv_rate",
                                 "srv_diff_host_rate")

host_based_traffic_features <- c("dst_host_count",
                                 "dst_host_srv_count",
                                 "dst_host_same_srv_rate",
                                 "dst_host_diff_srv_rate",
                                 "dst_host_same_src_port_rate",
                                 "dst_host_srv_diff_host_rate",
                                 "dst_host_serror_rate",
                                 "dst_host_srv_serror_rate",
                                 "dst_host_rerror_rate",
                                 "dst_host_srv_rerror_rate")

# These are the final column names as specified in the original CSV files
col_names <- c(basic_features, content_features, time_based_traffic_features, host_based_traffic_features, "label", "difficulty")

if (file.exists("data/NSL-KDD.RData")) {
  load("data/NSL-KDD.RData")
} else {
  nsl_kdd_train_csv = read.csv(unz("data/NSL-KDD.zip", "KDDTrain+.txt"), col.names = col_names, header = FALSE)
  nsl_kdd_test_csv = read.csv(unz("data/NSL-KDD.zip", "KDDTest+.txt"), col.names = col_names, header = FALSE)
  # Remove difficulty column
  nsl_kdd_train_csv <- nsl_kdd_train_csv %>% select(-difficulty)
  nsl_kdd_test_csv <- nsl_kdd_test_csv %>% select(-difficulty)
  save(nsl_kdd_train_csv, nsl_kdd_test_csv, file = "data/NSL-KDD.RData")
}

prepare_data <- function(binary_classification, nsl_kdd_train_csv, nsl_kdd_test_csv) {
  if (binary_classification) {
    train_data <- nsl_kdd_train_csv %>%
                    mutate(label = ifelse(label == "normal", "X2", "X1"))
    train_data$label <- factor(train_data$label)
    test_data <- nsl_kdd_test_csv %>%
                  mutate(label = ifelse(label == "normal", "X2", "X1"))
    test_data$label <- factor(test_data$label)
  } else {
    dos_attacks = c("neptune", "back", "land", "pod", "smurf", "teardrop", "mailbomb", "apache2", "processtable", "udpstorm", "worm")
    probing_attacks = c("ipsweep", "nmap", "portsweep", "satan", "mscan", "saint")
    r2l_attacks = c("ftp_write", "guess_passwd", "imap", "multihop", "phf", "spy", "warezclient", "warezmaster", "sendmail", "named", "snmpgetattack", "snmpguess", "xlock", "xsnoop", "httptunnel")
    u2r_attacks = c("buffer_overflow", "loadmodule", "perl", "rootkit", "ps", "sqlattack", "xterm")
    
    label_to_multi_category <- function(label) {
      if (label == "normal") {
        return("X5")
      } else if (label %in% dos_attacks) {
        return("X4")
      } else if (label %in% probing_attacks) {
        return("X3")
      } else if (label %in% r2l_attacks) {
        return("X2")
      } else if (label %in% u2r_attacks) {
        return("X1")
      }
    }
    train_data <- nsl_kdd_train_csv %>%
                    mutate(label = sapply(label, label_to_multi_category))
    train_data$label <- factor(train_data$label)
    test_data <- nsl_kdd_test_csv %>%
                  mutate(label = sapply(label, label_to_multi_category))
    test_data$label <- factor(test_data$label)
    
    rm(dos_attacks, probing_attacks, r2l_attacks, u2r_attacks, label_to_multi_category)
  }
  return(list("train" = train_data, "test" = test_data))
}
binary_class_data <- prepare_data(TRUE, nsl_kdd_train_csv, nsl_kdd_test_csv)
multi_class_data <- prepare_data(FALSE, nsl_kdd_train_csv, nsl_kdd_test_csv)
```

NSL-KDD dataset [@tavallaee2009detailed] is one of the most effective datasets in the domain of intrusion detection. It is a modified version of KDDCUP'99 dataset [@kddCup], which was created in 1999. KDDCUP'99 is constructed from simulated TCP connections in a military network environment [@bamakan2016effective]. KDDCUP'99 had been the most widely used dataset to evaluate IDSs until recent years [@salo2018data]. However, researchers found some deficiencies that make it less reliable [@tavallaee2009detailed]:

1. Redundant records: this mainly affects the performance of any classifier such that it is biased towards more frequent records.
2. Low difficulty level: applying simple machine learning methods will give at least 86% accuracy, which makes it difficult to compare the different models as they will fall in the range of 86% to 100%.

To deal with these deficiencies, the following improvements were applied to NSLKDD [@tavallaee2009detailed]:

1. Removing all redundant records from train and test sets so that there will be no biasing.
2. Better sampling and distribution for the records which will increase the classification challenge.
3. Reasonable number of records in train and test sets. This makes it affordable to run experiments on the whole dataset without any need for sampling.

NSL-KDD still does not perfectly represent real networks. Nonetheless, it is still a reliable benchmark dataset to compare intrusion detection methods.

## Attacks Categories
NSL-KDD records are labeled as normal or attack. There are 39 different attacks distributed (with some overlap) as 22 attacks in the training set and 37 attacks in the testing set. These attacks fall into four basic categories detailed as follows:

* Denial of Service Attack (DoS): involves attacks which try to keep the machine's memory or computing resources too busy such that the machine cannot serve its legitimate users.
* User to Root Attack (U2R): involves attacks in which the attacker first gains access to a normal user account, and then tries to exploit some vulnerability to gain root access to the system.
* Remote to Local Attack (R2L): involves attacks in which attacker keeps sending packets to a machine over some network. The main purpose in these attacks is to try to find a system vulnerability to gain access as a normal user.
* Probing Attack: these attacks scan the computer networks to find some vulnerability in its security controls.

Table \@ref(tab:nsl-attacks-tab) shows the detailed distribution of the different attacks.

```{r Add kableExtra, echo=FALSE, message=FALSE, warning=FALSE}
options(knitr.table.format = "latex")
library(kableExtra)
```

```{r nsl-attacks-tab, echo=FALSE, message=FALSE, warning=FALSE}
dos_attacks = c("neptune", "back", "land", "pod", "smurf", "teardrop", "mailbomb", "apache2", "processtable", "udpstorm", "worm")
probing_attacks = c("ipsweep", "nmap", "portsweep", "satan", "mscan", "saint")
r2l_attacks = c("ftp_write", "guess_passwd", "imap", "multihop", "phf", "spy", "warezclient", "warezmaster", "sendmail", "named", "snmpgetattack", "snmpguess", "xlock", "xsnoop", "httptunnel")
u2r_attacks = c("buffer_overflow", "loadmodule", "perl", "rootkit", "ps", "sqlattack", "xterm")

attacks_categories <- c(paste(dos_attacks, collapse = ", "), paste(probing_attacks, collapse = ", "), paste(r2l_attacks, collapse = ", "), paste(u2r_attacks, collapse = ", "))
attacks_df <- data.frame(c("DoS", "Probing", "R2L", "U2R"), attacks_categories)
colnames(attacks_df) <- c("Attack Category", "Attacks Included")
attacks_df %>% kbl(caption = "NSL-KDD Attacks Categories") %>%
  kable_styling(latex_options = c("HOLD_position"), position = "center") %>%
  column_spec(1, border_left = T) %>%
  column_spec(2, width = "30em",border_right = T) %>%
  row_spec(0, bold = T)
```

Figure \@ref(fig:nsl-attacks-fig) shows the distribution of normal and attack connections in NSL-KDD training and testing sets. It is clear that the training set is divided into ~50% normal connections and ~50% for the attacks. Moreover, DoS attack category is dominating the attacks the training set, while R2L and U2R attack categories have few connections. Consequently, it will be challenging to correctly classify R2L and U2R records.

## Features Description
Moreover, NSL-KDD is constructed from 41 attributes or features. These features fall into three main categories as shown in Table \@ref(tab:nsl-attrs-tab):

1. Basic features: these attributes are extracted from a TCP/IP connection.
2. Content features: these attributes are extracted from the data portion of the packet. They are very important to detect R2L and U2R attacks. This is because these attacks usually involve a single connection.
3. Traffic features:
    + Time-based traffic features: these attributes are extracted from connections in the past two seconds that have the same destination or same service as current connection.
    + Connection-based traffic features: these attributes are extracted from last 100 connections that has the same destination or same service as current connection. Extracting such attributes contributes more in detecting probing attacks.

```{r nsl-attacks-fig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "Distribution of Normal/Attacks in NSL-KDD Training and Testing Sets"}
training_multi_label <- multi_class_data$train %>%
                          select(label) %>%
                          mutate(set = "Training Set")
testing_multi_label <- multi_class_data$test %>%
                          select(label) %>%
                          mutate(set = "Testing Set")
rbind(training_multi_label, testing_multi_label) %>%
  mutate(multi_label = case_when(label == "X1" ~ "U2R",
                                  label == "X2" ~ "R2L",
                                  label == "X3" ~ "Probing",
                                  label == "X4" ~ "DoS",
                                  label == "X5" ~ "Normal")) %>%
  ggplot(aes(x = fct_infreq(multi_label), group = set)) +
  geom_bar(aes(y = ..prop..), stat = "count") +
  geom_label(aes(label = scales::percent(..prop..), y = ..prop..), stat = "count", vjust = "outward", size = 3.5) +
  geom_label(aes(label = ..count.., y = ..prop..), stat = "count", vjust = "inward", size = 3.5) +
  facet_grid(~factor(set, levels = c("Training Set", "Testing Set"))) +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Label", y = "Frequency")
```

```{r nsl-attrs-tab, echo=FALSE, message=FALSE, warning=FALSE}
features_df_transformer <- function(features_names, category_name) {
  custom_col_summary <- function(col) {
    # if the column is not numeric, then we will print all its unique values separated by a comma
    # if the column is numeric and has only two values, we will print those values separated by a comma
    # if the column is numeric and has many values, we will print it as min-max
    return(tibble("Type" = class(col), "Range or Values" = ifelse(is.numeric(col), paste(min(col), max(col), sep = ifelse(length(unique(col)) == 2, ",", "-")), paste(unique(col), collapse = ", "))))
  }
  # we replicated the category to use it in collapse_rows in kbl
  df <- cbind("Category" = replicate(length(features_names), category_name), "Feature" = features_names, bind_rows(lapply(X = nsl_kdd_train_csv[, features_names], FUN = custom_col_summary)))
  df
}
  
basic_df <- features_df_transformer(basic_features, "Basic features")
content_df <- features_df_transformer(content_features, "Content features")
time_based_traffic_df <- features_df_transformer(time_based_traffic_features, "Time-based traffic features")
host_based_traffic_df <- features_df_transformer(host_based_traffic_features, "Connection-based traffic features")
atts_df <- rbind(basic_df, content_df, time_based_traffic_df, host_based_traffic_df)

atts_df %>% kbl(caption = "NSL-KDD Attributes", col.names = c("Category", "Feature", "Type", "Range/Values"), centering = F) %>%
    kable_styling(position = "left", latex_options = c("HOLD_position")) %>%
    column_spec(1, width = "4.8em", border_left = T) %>%
    column_spec(2, width = "13.5em") %>%
    column_spec(3, width = "3.7em") %>%
    column_spec(4, width = "26.5em", border_right = T) %>%
    row_spec(0, bold = T) %>%
    collapse_rows(1, valign = "middle")
```

# Methodology

We will build two IDSs, one for binary classification and the other for multi-class classification. So, the training and testing sets for the first system will have the `label` as `Normal` or `Attack`, while for the second system the `label` will be `Normal` or `DoS` or `Probing` or `R2L` or `U2R`. The performance of each system will be evaluated using six classifiers: Recursive Partitioning, Naive Bayes, KNN, SVM, Random Forest, and Multi-Layer Perceptron. Each classifier needs to be tuned before training it on the whole training set. Therefore, we chose to apply a tune grid search with 10-fold cross validation on each pair of system and classifier. As a result, we will have the best tuned parameters for each classifier to the relevant system. Then, we will train the best tuned classifiers on the relevant training set. Finally, we will apply the prediction on the relevant testing set, and we will report the performance metrics. The following algorithm summarizes the steps followed to tune any classifier.

\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
Define sets of model parameter values to evaluate\;
Shuffle training set randomly\;
Split training set into 10 groups\;
\For{each parameter set}{
    \For{each group}{
        Take the group as a hold out or test data set\;
        Take the other 9 groups as a training data set\;
        Preprocess the training data\;
        Fit the model on the training data\;
        Predict the hold out data\;
    }
    Calculate the average performance across predictions of hold out data\;
}
Determine the optimal parameter set\;
Preprocess the whole training set\;
Fit the final model to the whole training set using the optimal parameter set\;
\caption{Tune Grid Search with 10-fold Cross-Validation Algorithm}
\end{algorithm}

In the next sections, we will go through the preprocessing stage, the classifiers used, and the evaluation metrics.

## Preprocessing Stage

As noticed from the previous algorithm, we preprocess the data before any training step. This includes both the intermediary training steps performed within cross validation, and the final training step using the optimal tuned parameters. Moreover, the result of the preprocessing stage will be preserved to apply it on any prediction step. For example, if in the preprocessing of the first group `is_host_login` feature was removed as it has zero-variance in that training group, the same feature will be removed from the relevant testing group. Note that removing `is_host_login` feature from one of the groups does not mean it will be removed from the whole training set, because in the full training set it might not have zero-variance.

We chose to apply three main steps in the preprocessing stage which are: removing zero-variance numerical features, normalizing the rest of numerical features, and transforming the nominal features to be numerical. To apply these steps, we decided to use `recipes` package [@max2021recipes] which provides powerful preprocessing capabilities. In the next subsections, we will explain briefly each of these steps.

### Remove Zero-Variance Numerical Features
A zero-variance feature is a feature that has only one static value. These features are usually removed before the training because they are seen as having no impact on the output. To apply removing zero-variance features, we used `step_zv` function from `recipes` package. This is an example of how to use `step_zv`.

```{r Example of step_zv usage, eval=FALSE}
step_zv(all_numeric_predictors())
```

### Normalize Numerical Features using Min-Max

Another important step before working with classification algorithms is to normalize numeric features. Normalizing a feature means to scale its values to fall into a smaller range. For example, there are features in NSL-KDD dataset that have wide range of values, such as: `duration`, `src_bytes` and `num_root`. While there are other features that have smaller range of values, such as: `num_failed_logins`, `is_host_login`, and `srv_count`. Keeping the features without normalization may cause biasing towards selecting wide range features which may also affect classification performance. To prevent this dominance, we chose to scale all the numeric features to fall in the range of 0-1 using min-max normalization. Min-max scaling is shown in Equation \@ref(eq:minmax), where $x$ is the value to be scaled in feature $X$, $MinMax(x)$ is the scaled value of $x$, $Min(X)$ and $Max(X)$ are the minimum and maximum values respectively in feature $X$, $min$ and $max$ are the boundaries of the new range.

\begin{equation} 
  MinMax(x) = min + (max - min)(\frac{x - Min(X)}{Max(X) - Min(X)})
  (\#eq:minmax)
\end{equation} 

To apply normalizing numerical features, we used `step_range` function from `recipes` package. This is an example of how to use `step_range`.

```{r Example of step_range usage, eval=FALSE}
step_range(all_numeric_predictors())
```

### Transform Nominal Features using Probability Density Function (PDF)

Many classification algorithms are mathematical-based. Therefore, it is important to transform the nominal features of a dataset into their numerical representation. NSL-KDD dataset has three nominal features (as stated in Table \@ref(tab:nsl-attrs-tab)): `protocol_type`, `service`, and `flag`.

To avoid biasing the data, we did not encode the data with a static value map (e.g. http takes 1, smtp takes 2, and so on). Rather, we applied probability density function as in Equation \@ref(eq:pdf) [@seth2016intrusion] such that the most frequent nominal value in a column takes the highest numerical value while still being bounded between 0 and 1. This range goes along with the numerical features normalization.

\begin{equation} 
  PDF(x) = \frac{occur(x)}{n}
  (\#eq:pdf)
\end{equation} 

where $occur(x)$ is the number of occurrences of value $x$ within a column, and $n$ is the total number of records.

To apply transforming nominal features, we had to build a custom `step` according to official `recipes` documentation ^[http://cran.nexr.com/web/packages/recipes/vignettes/Custom_Steps.html] as shown below. We called this step `step_nominalpdf`.

```{r Normalize PDF Recipe Step Definition}
##########################################################
# Normalize PDF Recipe Step Definition
# References: http://cran.nexr.com/web/packages/recipes/vignettes/Custom_Steps.html
#             https://github.com/tidymodels/recipes/blob/main/R/center.R
##########################################################

step_nominalpdf <-
  function(recipe,
           ..., 
           role = NA,
           trained = FALSE,
           ref_dist = NULL,
           skip = FALSE,
           id = rand_id("nominalpdf")) {
    add_step(
      recipe, 
      step_nominalpdf_new(
        terms = enquos(...), 
        trained = trained,
        role = role, 
        ref_dist = ref_dist,
        skip = skip,
        id = id
      )
    )
}

step_nominalpdf_new <-
  function(terms, role, trained, ref_dist, skip, id) {
    step(
      subclass = "nominalpdf", 
      terms = terms,
      role = role,
      trained = trained,
      ref_dist = ref_dist,
      skip = skip,
      id = id
    )
}

prep.step_nominalpdf <- function(x, training, info = NULL, ...) {
  col_names <- recipes_eval_select(x$terms, training, info)
  
  ref_dist <- list()
  train_ln <- nrow(training)
  for (i in col_names) {
    # For each column, table will return the count of each value
    # to normalize that count, we divide it by the number of rows
    # For example, if we have (a, b, a, c, d) in a column
    # The output will be a table like this
    #   a   b   c   d 
    # 0.4 0.2 0.2 0.2
    ref_dist[[i]] <- table(training[, i]) / train_ln
  }
  
  ## Always return the updated step
  step_nominalpdf_new(
    terms = x$terms,
    role = x$role,
    trained = TRUE,
    ref_dist = ref_dist,
    skip = x$skip,
    id = x$id
  )
}

pdf_by_ref <- function(x, ref) {
  # if we have the following values in ref:
  #   a   b   c   d 
  # 0.4 0.2 0.2 0.2
  # And we got x = "a", the function will return 0.4
  # if we got x = "e", the function will return 0
  ifelse(x %in% names(ref), ref[x][[1]], 0)
}

bake.step_nominalpdf <- function(object, new_data, ...) {
  require(tibble)
  vars <- names(object$ref_dist)
  
  # Transform the columns
  for(i in vars) {
    new_data[, i] <- apply(new_data[, i], 1, pdf_by_ref, ref = object$ref_dist[[i]])
  }
  ## Always convert to tibbles on the way out
  tibble::as_tibble(new_data)
}

print.step_nominalpdf <- function(x, width = max(20, options()$width - 30), ...) {
  cat("PDF for ", sep = "")
  printer(names(x$ref_dist), x$terms, x$trained, width = width)
  invisible(x)
}

tidy.step_nominalpdf <- function(x, ...) {
  if (is_trained(x)) {
    res <- tibble(terms = names(x$ref_dist),
                  value = unname(x$ref_dist))
  } else {
    term_names <- sel2char(x$terms)
    res <- tibble(terms = term_names,
                  value = na_dbl)
  }
  res$id <- x$id
  res
}
```

This is an example of how to use `step_nominalpdf`.

```{r Example of step_nominalpdf usage, eval=FALSE}
step_nominalpdf(all_nominal_predictors())
```

## Classifiers {#methodology-classifiers}

In order to perform classification, we used `caret` package [@max2021caret] which provides predefined classification algorithms in a convenient way. Table \@ref(tab:classifiers-desc-tab) summarizes the classifiers we used indicating the name of the classifier in `caret` along with its original package (which can be `caret` itself as for `knn`).

(ref:terry2019Rpart) [@terry2019Rpart]
(ref:michal2019naivebayes) [@michal2019naivebayes]
(ref:alexandros2004kernlab) [@alexandros2004kernlab]
(ref:andy2002rf) [@andy2002rf]
(ref:christoph2012mlp) [@christoph2012mlp]
```{r classifiers-desc-tab, echo=FALSE, message=FALSE, warning=FALSE}
classifiers_desc <- data.frame(c("Recursive Partitioning", "Naive Bayes", "KNN", "SVM", "Random Forest", "Multi-Layer Perceptron"), c("rpart", "naive_bayes", "knn", "svmLinear", "parRF", "mlp"), c("rpart::rpart (ref:terry2019Rpart)", "naivebayes::naive_bayes (ref:michal2019naivebayes)", "caret::knn3", "kernlab::ksvm (ref:alexandros2004kernlab)", "randomForest::randomForest (ref:andy2002rf)", "RSNNS::mlp (ref:christoph2012mlp)"), c("cp", "laplace, usekernel, adjust", "k", "C", "mtry", "size"))

classifiers_desc %>% kbl(caption = "Classifiers Description", col.names = c("Classifier Name", "Caret Name", "Package::Function", "Parameters")) %>%
    kable_styling(latex_options = c("HOLD_position")) %>%
    column_spec(1, border_left = T) %>%
    column_spec(4, border_right = T) %>%
    row_spec(0, bold = T)
```

There is a great integration between `caret` and `recipes` packages which makes it too easy to build the whole training flow in few lines. Also, `caret` has strong support for parallel execution ^[http://topepo.github.io/caret/parallel-processing.html]. Here is a full example that uses `caret`, `recipes` and `doParallel` package [@microsoft2020parallel].

```{r Full example on the flow using caret and recipes and doParallel, eval=FALSE}
# Define the recipe for the preprocessing steps
data_rec <- recipe(label ~ ., data = nsl_training_data) %>%
              step_zv(all_numeric_predictors()) %>%
              step_range(all_numeric_predictors()) %>%
              step_nominalpdf(all_nominal_predictors())
# Create a cluster
cluster = makePSOCKcluster(detectCores() - 2)
# Register the cluster
registerDoParallel(cluster)
# Register the functions related to step_nominalpdf to the cluster
clusterExport(cl=cluster, varlist=c("step_nominalpdf", "step_nominalpdf_new", 
                                  "prep.step_nominalpdf", "pdf_by_ref", 
                                  "bake.step_nominalpdf", 
                                  "print.step_nominalpdf", 
                                  "tidy.step_nominalpdf"), envir=environment())
set.seed(123)
# Train KNN on NSL-KDD data taking the recipe as input (default k for cv is 10)
# Use the default tune grid values for k (the tuning parameter of knn)
model_fit <- train(x = data_rec,
               data = nsl_training_data,
               method = "knn",
               trControl = trainControl(method = 'cv'))
stopCluster(cluster)
```

Also, `caret` supports convenient abstract way to perform prediction and compute the confusion matrix such as the following example.

```{r Example on the prediction using caret, eval=FALSE}
model_pred <- predict(trained_model, testing_data)
confusionMatrix(model_pred, testing_data$label)
```

## Evaluation Metrics

The effectiveness of any IDS is mainly measured by [@bamakan2016effective] [@hodo2017shallow] [@hamed2018survey]: overall accuracy, detection rate, false alarm rate, and training time. A well-performing IDS would achieve a low false alarm rate, and high accuracy and detection rate. The common way to derive the definition of these metrics is through a confusion matrix.

### Metrics for Binary Classification IDS {#binary-metrics}

In a binary classification IDS, a record that is labeled as "attack" is a "Positive" record, and a record that is labeled as "normal" is a "Negative" record. Confusion matrix is a two by two matrix that represents the four possible combinations of the actual records and the predicted records.

```{=latex}
\begin{table}[htb]
\centering
\caption{Confusion matrix}
\label{tab:confusion}
\begin{tabular}{l|l|c|c|}
\multicolumn{2}{c}{}&\multicolumn{2}{c}{Predicted}\\
\cline{3-4}
\multicolumn{2}{c|}{}&Negative (normal)&Positive (attack)\\
\cline{2-4}
\multirow{2}{*}{Actual}& Negative (normal) & $TN$ & $FP$\\
\cline{2-4}
& Positive (attack) & $FN$ & $TP$\\
\cline{2-4}
\end{tabular}
\end{table}
```

Table \@ref(tab:confusion) shows a confusion matrix where:

* True Negative (TN): represents the number of normal records correctly predicted as normal.
* False Positive (FP): represents the number of attack records wrongly predicted as normal.
* False Negative (FN): represents the number of normal records wrongly predicted as attack.
* True Positive (TP): represents the number of attack records correctly predicted as attack.

Based on the confusion matrix, we can define the metrics mentioned above as follows:

* Overall Accuracy: is the percent of correctly classified records. It is calculated by Equation \@ref(eq:accuracy).
\begin{equation} 
  \text{Overall Accuracy} = \frac{TN + TP}{TN + FP + FN + TP}
  (\#eq:accuracy)
\end{equation}
* Detection Rate (DR): also called Recall or sensitivity or true positive rate (TPR). It is the percent of correctly classified attacks to the total number of actual attacks. When it is near 1, it means that the classifier performed well in predicting almost all actual attacks. It is calculated by Equation \@ref(eq:dr).
\begin{equation} 
  \text{Detection Rate (DR)} = \frac{TP}{TP + FN}
  (\#eq:dr)
\end{equation}
* False Alarm Rate (FAR): also called False Positive Rate (FPR). It is the percentage of wrongly classified normal records. When it is near zero, it means that the classifier performed well in avoiding misprediction of almost all normal records. It is calculated by Equation \@ref(eq:far).
\begin{equation} 
  \text{FAR} = \frac{FP}{FP + TN}
  (\#eq:far)
\end{equation}

### Metrics for Multi-class Classification IDS {#multi-metrics}

In a multi-class IDS, all metrics except overall accuracy need to be calculated per class. Therefore, we used a strategy called one-vs-rest, which treats each class as if it is the positive class that we want to detect, and the other classes are negative. We demonstrate this treatment by having an example of a 5x5 confusion matrix on NSL-KDD classes. This confusion matrix is shown in Table \@ref(tab:multi-conf-matrix).

```{=latex}
\begin{table}[htb]
    \centering
    \caption{Multi-class confusion matrix}
    \label{tab:multi-conf-matrix}
    \begin{tabular}{l|l|c|c|c|c|c|}
    \multicolumn{3}{c}{}&\multicolumn{2}{c}{Predicted}&\multicolumn{2}{c}{}\\
    \cline{3-7}
    \multicolumn{2}{c|}{} & Normal & DoS & Probing & R2L & U2R\\
    \cline{2-7}
    \multirow{5}{*}{Actual}& Normal & 21761 & 286 & 677 & 287 & 106\\
    \cline{2-7}
    & DoS & 1183 & 14535 & 209 & 74 & 15\\
    \cline{2-7}
    & Probing & 510 & 72 & 3550 & 68 & 23\\
    \cline{2-7}
    & R2L & 631 & 77 & 197 & 235 & 24\\
    \cline{2-7}
    & U2R & 31 & 1 & 1 & 2 & 1\\
    \cline{2-7}
    \end{tabular}
\end{table}
```

We will break this confusion matrix into 5 smaller matrices each of size 2x2. Tables \@ref(tab:confusion-normal)-\@ref(tab:confusion-u2r) show the resulting confusion matrices for each class in NSL-KDD. And now, we can easily calculate the metrics as follows:

* Overall Accuracy:
\begin{equation*} 
\frac{21761 + 14535 + 3550 + 235 + 1}{44556}=\frac{40082}{44556}=0.8996
\end{equation*}
* Normal DR:
\begin{equation*} 
\frac{21761}{21761 + 1356}=\frac{21761}{23117}=0.9413
\end{equation*}
* DoS DR: 
\begin{equation*} 
\frac{14535}{14535 + 1481}=\frac{14535}{16016}=0.9075
\end{equation*}
* Probing DR:
\begin{equation*} 
\frac{3550}{3550 + 673}=\frac{3550}{4223}=0.8407
\end{equation*}
* R2L DR:
\begin{equation*} 
\frac{235}{235 + 929}=\frac{235}{1164}=0.2019
\end{equation*}
* U2R DR:
\begin{equation*} 
\frac{1}{1 + 35}=\frac{1}{36}=0.0278
\end{equation*}
* Normal FAR:
\begin{equation*} 
\frac{2355}{2355 + 19084}=\frac{2355}{21439}=0.1098
\end{equation*}
* DoS FAR:
\begin{equation*} 
\frac{436}{436 + 28104}=\frac{436}{28540}=0.0153
\end{equation*}
* Probing FAR:
\begin{equation*} 
\frac{1084}{1084 + 39249}=\frac{1084}{40333}=0.0269
\end{equation*}
* R2L FAR:
\begin{equation*} 
\frac{431}{431 + 42961}=\frac{431}{43392}=0.0099
\end{equation*}
* U2R FAR:
\begin{equation*} 
\frac{168}{168 + 44352}=\frac{168}{44520}=0.0037
\end{equation*}

```{=latex}
\begin{table}[!ht]
\begin{minipage}{0.5\textwidth}
\caption{Normal Confusion matrix}
\label{tab:confusion-normal}
\begin{tabular}{l|l|c|c|}
\multicolumn{2}{c}{}&\multicolumn{2}{c}{Predicted}\\
\cline{3-4}
\multicolumn{2}{c|}{}&Other&Normal\\
\cline{2-4}
\multirow{2}{*}{Actual}& Other & 19084 & 2355\\
\cline{2-4}
& Normal & 1356 & 21761\\
\cline{2-4}
\end{tabular}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\caption{DoS Confusion matrix}
\label{tab:confusion-dos}
\begin{tabular}{l|l|c|c|}
\multicolumn{2}{c}{}&\multicolumn{2}{c}{Predicted}\\
\cline{3-4}
\multicolumn{2}{c|}{}&Other&DoS\\
\cline{2-4}
\multirow{2}{*}{Actual}& Other & 28104 & 436\\
\cline{2-4}
& DoS & 1481 & 14535\\
\cline{2-4}
\end{tabular}
\end{minipage}
\par\vspace{2cm}
\begin{minipage}{0.5\textwidth}
\caption{Probing Confusion matrix}
\label{tab:confusion-probing}
\begin{tabular}{l|l|c|c|}
\multicolumn{2}{c}{}&\multicolumn{2}{c}{Predicted}\\
\cline{3-4}
\multicolumn{2}{c|}{}&Other&Probing\\
\cline{2-4}
\multirow{2}{*}{Actual}& Other & 39249 & 1084\\
\cline{2-4}
& Probing & 673 & 3550\\
\cline{2-4}
\end{tabular}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\caption{R2L Confusion matrix}
\label{tab:confusion-r2l}
\begin{tabular}{l|l|c|c|}
\multicolumn{2}{c}{}&\multicolumn{2}{c}{Predicted}\\
\cline{3-4}
\multicolumn{2}{c|}{}&Other&R2L\\
\cline{2-4}
\multirow{2}{*}{Actual}& Other & 42961 & 431\\
\cline{2-4}
& R2L & 929 & 235\\
\cline{2-4}
\end{tabular}
\end{minipage}
\par\vspace{2cm}
\begin{minipage}{\textwidth}
\centering
\caption{U2R Confusion matrix}
\label{tab:confusion-u2r}
\begin{tabular}{l|l|c|c|}
\multicolumn{2}{c}{}&\multicolumn{2}{c}{Predicted}\\
\cline{3-4}
\multicolumn{2}{c|}{}&Other&U2R\\
\cline{2-4}
\multirow{2}{*}{Actual}& Other & 44352 & 168\\
\cline{2-4}
& U2R & 35 & 1\\
\cline{2-4}
\end{tabular}
\end{minipage}
\end{table}
```

```{r Classifiers definition and training and prediction functions, echo=FALSE, message=FALSE, warning=FALSE}
classifiers <- c("rpart", "naive_bayes", "knn", "svmLinear", "parRF", "mlp")

classifier_trained_model <- function(model_name, training_data) {
  message(paste(model_name, length(levels(training_data$label))))
  suffix <- ifelse(length(levels(training_data$label)) == 2, "binary", "multi")
  saved_model_path <- paste("data/", paste(model_name, suffix, "fit", sep = "_"), ".rds", sep = "")
  if (file.exists(saved_model_path)) {
    message(paste(model_name, "Reading already trained model from:", saved_model_path))
    model_fit <- readRDS(saved_model_path)
  } else {
    message(paste(model_name, "Did not find a saved model in: ", saved_model_path))
    data_rec <- recipe(label ~ ., data = training_data) %>%
                  step_zv(all_numeric_predictors()) %>%
                  step_range(all_numeric_predictors()) %>%
                  step_nominalpdf(all_nominal_predictors())
    # Create a cluster
    cluster = makePSOCKcluster(detectCores() - 2)
    # Register the cluster
    registerDoParallel(cluster)
    # Register the functions related to step_nominalpdf to the cluster
    clusterExport(cl=cluster, varlist=c("step_nominalpdf", "step_nominalpdf_new", "prep.step_nominalpdf", "pdf_by_ref", "bake.step_nominalpdf", "print.step_nominalpdf", "tidy.step_nominalpdf"), envir=environment())
    
    model_fit <- tryCatch({
        set.seed(123)
        # Setting the seeds to NULL, this way caret will automatically generate the seeds for 'cv' based on the seed we set
        # Use the default tune grid values offered from caret for each classifier
        model_fit <- train(x = data_rec,
                           data = training_data,
                           method = model_name,
                           trControl = trainControl(method = 'cv', seeds = NULL, summaryFunction = multiClassSummary, classProbs = TRUE))
        saveRDS(model_fit, saved_model_path)
        model_fit
      },
      error = function(cond) {
        message(paste(model_name, "Here's the original error message:", cond))
        return(NULL)
      },
      finally = {
        stopCluster(cluster)
      }
    )
  }
  return(model_fit)
}

classifier_predict <- function(trained_model, testing_data) {
  model_name <- trained_model$method
  suffix <- ifelse(length(levels(testing_data$label)) == 2, "binary", "multi")
  saved_pred_path <- paste("data/", paste(model_name, suffix, "preds", sep = "_"), ".rds", sep = "")
  if (file.exists(saved_pred_path)) {
    message(paste(model_name, "Reading already predicted values from:", saved_pred_path))
    model_pred <- readRDS(saved_pred_path)
  } else {
    message(paste(model_name, "Did not find saved preds in: ", saved_pred_path))
    
    # Create a cluster
    cluster = makePSOCKcluster(detectCores() - 2)
    # Register the cluster
    registerDoParallel(cluster)
    # Register the functions related to step_nominalpdf to the cluster
    clusterExport(cl=cluster, varlist=c("step_nominalpdf", "step_nominalpdf_new", "prep.step_nominalpdf", "pdf_by_ref", "bake.step_nominalpdf", "print.step_nominalpdf", "tidy.step_nominalpdf"), envir=environment())
    
    model_pred <- tryCatch({
        set.seed(123)
        if (model_name == "naive_bayes") {
          library(naivebayes)
        } else if (model_name == "parRF") {
          library(randomForest)
        } else {
          library(caret)
        }
        model_pred <- predict(trained_model, testing_data)
        saveRDS(model_pred, saved_pred_path)
        model_pred
      },
      error = function(cond) {
        message(paste(model_name, "Here's the original error message:", cond))
        return(NULL)
      },
      finally = {
        stopCluster(cluster)
      }
    )
  }
  if (!is.null(model_pred)) {
    CM <- tryCatch({
        confusionMatrix(model_pred, testing_data$label)
      },
      error = function(cond) {
        message(paste(model_name, "Here's the original error message:", cond))
        return(NULL)
      }
    )
    return(CM)
  }
  return(NULL)
}
```

\newpage

# Binary Classification IDS Analysis and Results

We applied the flow proposed in Section \@ref(methodology-classifiers) to the six filters mentioned in Table \@ref(tab:classifiers-desc-tab) with the default tuning grid values from `caret` on the binary classification IDS. In this section, we will analyze the binary classification IDS training and its performance results.

```{r Train the classifiers for binary classification, echo=FALSE, message=FALSE, warning=FALSE}
binary_class_models <- lapply(classifiers, classifier_trained_model, training_data = binary_class_data$train)
```

## Binary Classification Training Analysis

Table \@ref(tab:binary-tuning-tab) shows the parameters tuning results for each classifier on the binary classification IDS. The accuracy in this table is computed as the average of the 10-folds for the best tuned parameters, it is not for the final model. Also, it is noticeable that random forest and SVM classifiers took the most significant time to tune the parameters.

```{r binary-tuning-tab, echo=FALSE, message=FALSE, warning=FALSE}
tuning_table <- function(model) {
  return(tibble(method = model$method,
             parameters = paste(paste(colnames(model$bestTune), model$bestTune, sep = " = "), collapse = ", "),
             accuracy = model$results[rownames(model$bestTune), c("Accuracy")],
             tuning = model$times$everything[[3]] - model$times$final[[3]]))
}

bind_rows(lapply(X = binary_class_models, FUN = tuning_table)) %>%
  kbl(caption = "Binary Classification IDS Parameters Tuning", col.names = c("Classifier", "Best Tuned Parameters", "Accuracy", "Tuning Time (seconds)")) %>%
  kable_styling(latex_options = c("HOLD_position"), position = "center") %>%
  column_spec(1, border_left = T) %>%
  column_spec(4, border_right = T) %>%
  row_spec(0, bold = T)
```

Figure \@ref(fig:binary-training-times-fig) shows the training time of the best tuned binary classifiers. Again, random forest and SVM classifiers took much more time than other classifiers.

```{r binary-training-times-fig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "Training Time of Best Tuned Binary Classifiers", fig.width=6, fig.height=4}
times_transformer <- function(model) {
    return(tibble("classifier" = model$method, "everything" = model$times$everything[[3]], "final" = model$times$final[[3]]))
}
binary_classifiers_times <- bind_rows(lapply(X = binary_class_models, FUN = times_transformer))
binary_classifiers_times %>%
  ggplot(aes(x = classifier, y = final)) +
  geom_bar(stat = "identity") +
  geom_label(aes(label = sprintf('%0.5f', final))) +
  scale_y_log10() +
  labs(x = "Classifier", y = "Time (seconds)")
```

Some classifiers such as recursive partitioning `rpart` and random forest `parRF` classifiers have built-in feature importance calculation. Figures \@ref(fig:binary-var-rpart) and \@ref(fig:binary-var-rf) show the top five important features for `rpart` and `parRF` respectively. They actually share the same top feature which is `src_bytes`. But `rpart` gave more importance to other features as well although they are less important in `parRF`, such as `dest_bytes`.

```{r binary-var-rpart, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "Feature Importance for Binary Classification using rpart", fig.width=4, fig.height=4}
ggplot(varImp(binary_class_models[[1]]), top = 5)
```

```{r binary-var-rf, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "Feature Importance for Binary Classification using parRF", fig.width=4, fig.height=4}
ggplot(varImp(binary_class_models[[5]]), top = 5)
```

## Binary Classification Performance

We computed the metrics mentioned in Section \@ref(binary-metrics) using `confusionMatrix`. The accuracy of the binary classification IDS is shown in Figure \@ref(fig:binary-acc-fig). It is clear that `naive_bayes` had the worst performance with an accuracy of about 68%, while `parRF` had the best performance with an accuracy of about 79.8%. The other classifiers achieved almost the same accuracy as `parRF`.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
binary_class_preds <- lapply(binary_class_models, classifier_predict, testing_data = binary_class_data$test)

binary_metrics_tibble <- function(idx) {
    cls_TP <- binary_class_preds[[idx]]$table[1,1]
    cls_FP <- binary_class_preds[[idx]]$table[1,2]
    cls_FN <- binary_class_preds[[idx]]$table[2,1]
    cls_TN <- binary_class_preds[[idx]]$table[2,2]
    return(tibble(classifier = classifiers[idx], accuracy = binary_class_preds[[idx]]$overall["Accuracy"][[1]], TP = cls_TP, FP = cls_FP, FN = cls_FN, TN = cls_TN))
}

binary_metrics <- data.frame(bind_rows(lapply(X = seq_along(classifiers), FUN = binary_metrics_tibble))) %>%
        mutate(detection_rate = TP / (TP + FN),  far = FP / (FP + TN))
```

```{r binary-acc-fig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Accuracy of Binary Classfication classifiers", fig.width=6, fig.height=4}
binary_metrics %>%
  ggplot(aes(x = classifier, y = accuracy)) +
  geom_bar(stat = "identity") +
  geom_label(aes(label = sprintf('%0.5f', accuracy), fontface = ifelse(accuracy == max(accuracy), 2, 1)), vjust = "inward") +
  labs(x = "Classifier", y = "Accuracy")
```

In terms of the False Alarm Rate (FAR), Figure \@ref(fig:binary-far-fig) shows that `mlp` had the worst highest FAR of about 7.58%, while `naive_bayes` had the best lowest FAR of about 0.175% (but with low accuracy). Worths mentioning that `parRF` had the second lowest FAR of about 3.007% while achieving the highest accuracy at the same time.

```{r binary-far-fig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="False Alarm Rate (FAR) of Binary Classfication classifiers", fig.width=6, fig.height=4}
binary_metrics %>%
  ggplot(aes(x = classifier, y = far)) +
  geom_bar(stat = "identity") +
  geom_label(aes(label = sprintf('%0.5f', far), fontface = ifelse(far == invoke(pmin, na_if(far, 0), na.rm = TRUE), 2, 1)), vjust = "inward") +
  labs(x = "Classifier", "False Alarm Rate (FAR)")
```

In terms of the Detection Rate (DR), Figure \@ref(fig:binary-dr-fig) shows that `naive_bayes` had the worst DR of about 44.21%, while `parRF` had the highest DR of about 66.7%.

```{r binary-dr-fig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Detection Rate (DR) of Binary Classfication classifiers", fig.width=6, fig.height=4}
binary_metrics %>%
  ggplot(aes(x = classifier, y = detection_rate)) +
  geom_bar(stat = "identity") +
  geom_label(aes(label = sprintf('%0.5f', detection_rate), fontface = ifelse(detection_rate == max(detection_rate), 2, 1)), vjust = "inward") +
  labs(x = "Classifier", "Detection Rate (DR")
```

Overall, the best classifier for binary classification IDS is Random Forest `parRF` as it achieved a reasonable balance between accuracy, FAR and DR. The only difficulty with `parRF` is that it takes long amount of time to train the model.

# Multi-class Classification IDS Analysis and Results

We applied the flow proposed in Section \@ref(methodology-classifiers) to the six filters mentioned in Table \@ref(tab:classifiers-desc-tab) with the default tuning grid values from `caret` on the multi-class classification IDS.  In this section, we will analyze the multi-class classification IDS training and its performance results.

```{r Train the classifiers for multi-class classification, echo=FALSE, message=FALSE, warning=FALSE}
multi_class_models <- lapply(classifiers, classifier_trained_model, training_data = multi_class_data$train)
```

## Multi-class Classification Training Analysis

Table \@ref(tab:multi-tuning-tab) shows the parameters tuning results for each classifier on the multi-class classification IDS. The accuracy in this table is computed as the average of the 10-folds for the best tuned parameters, it is not for the final model. Similar to the binary classification IDS, `parRF` and `svmLinear` took much more time than other classifiers.

```{r multi-tuning-tab, echo=FALSE, message=FALSE, warning=FALSE}
bind_rows(lapply(X = multi_class_models, FUN = tuning_table)) %>%
  kbl(caption = "Multi-class Classification Tuning", col.names = c("Classifier", "Best Tuned Parameters", "Accuracy", "Tuning Time (seconds)")) %>%
  kable_styling(latex_options = c("HOLD_position"), position = "center") %>%
  column_spec(1, border_left = T) %>%
  column_spec(4, border_right = T) %>%
  row_spec(0, bold = T)
```

Figure \@ref(fig:multi-training-times-fig) shows the training time of the best tuned multi-class classifiers. Also here, random forest and SVM classifiers took much more time than other classifiers. Overall the training time of each classifier in multi-class classification is almost the same as binary classification time.

```{r multi-training-times-fig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "Training Time of Best Tuned Multi-class Classifiers", fig.width=6, fig.height=4}
multi_classifiers_times <- bind_rows(lapply(X = multi_class_models, FUN = times_transformer))
multi_classifiers_times %>%
  ggplot(aes(x = classifier, y = final)) +
  geom_bar(stat = "identity") +
  geom_label(aes(label = sprintf('%0.5f', final))) +
  scale_y_log10() +
  labs(x = "Classifier", y = "Time (seconds)")
```

Figures \@ref(fig:multi-var-rpart) and \@ref(fig:multi-var-rf) show the top five important features for `rpart` and `parRF` respectively. In this case, the order of important features is a bit different between `rpart` and `parRF`.

```{r multi-var-rpart, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "Feature Importance for Multi-class Classification using rpart", fig.width=4, fig.height=4}
ggplot(varImp(multi_class_models[[1]]), top = 5)
```

```{r multi-var-rf, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "Feature Importance for Binary Classification using parRF", fig.width=4, fig.height=4}
ggplot(varImp(multi_class_models[[5]]), top = 5)
```

## Multi-class Classification Performance

We computed the metrics mentioned in Section \@ref(multi-metrics) using `confusionMatrix`. The accuracy of the multi-class classification IDS is shown in Figure \@ref(fig:multi-acc-fig). It is clear that `naive_bayes` had the worst performance with an accuracy of about 52.46%, while `parRF` had the best performance with an accuracy of about 76.67%. The other classifiers achieved almost the same accuracy as `parRF`. Moreover, the accuracy of each classiffier in multi-class classification is less than its value in binary classification. This gives an indication that it is usually harder to build a multi-class classification IDS.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
multi_class_preds <- lapply(multi_class_models, classifier_predict, testing_data = multi_class_data$test)

accuracy_tibble <- function(idx) {
    return(tibble(classifier = classifiers[idx], accuracy = multi_class_preds[[idx]]$overall["Accuracy"][[1]]))
}

multi_acc <- data.frame(bind_rows(lapply(X = seq_along(classifiers), FUN = accuracy_tibble)))

metrics_calculator <- function(idx) {
    mtrx <- multi_class_preds[[idx]]$table
    mtrx_row_sums <- rowSums(mtrx)
    mtrx_col_sum <- colSums(mtrx)
    mtrx_diag_sum <- sum(diag(mtrx))
    mtrx_sum <- sum(mtrx)
    metric <- function(cls) {
        cls_TP <- mtrx[cls, cls]
        cls_FP <- mtrx_row_sums[cls][[1]] - mtrx[cls, cls]
        cls_FN <- mtrx_col_sum[cls][[1]] - mtrx[cls, cls]
        cls_TN <- mtrx_sum - (cls_TP + cls_FP + cls_FN)
        return(tibble(classifier = classifiers[idx], class = cls, TP = cls_TP, FP = cls_FP, FN = cls_FN, TN = cls_TN))
    }
    return(bind_rows(lapply(X = c("X1", "X2", "X3", "X4", "X5"), FUN = metric)))
}
multi_metrics <- data.frame(bind_rows(lapply(X = seq_along(classifiers), FUN = metrics_calculator))) %>%
        mutate(multi_label = case_when(class == "X1" ~ "U2R",
                                  class == "X2" ~ "R2L",
                                  class == "X3" ~ "Probing",
                                  class == "X4" ~ "DoS",
                                  class == "X5" ~ "Normal")) %>%
        mutate(detection_rate = TP / (TP + FN),  far = FP / (FP + TN)) %>%
        group_by(multi_label) %>%
        mutate(max_dr = max(detection_rate), min_far = invoke(pmin, na_if(far, 0), na.rm = TRUE))
```

```{r multi-acc-fig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Accuracy of Multi-class Classfication classifiers", fig.width=6, fig.height=4}
multi_acc %>%
  ggplot(aes(x = classifier, y = accuracy)) +
  geom_bar(stat = "identity") +
  geom_label(aes(label = sprintf('%0.5f', accuracy), fontface = ifelse(accuracy == max(accuracy), 2, 1)), vjust = "inward") +
  labs(x = "Classifier", y = "Accuracy")
```

Figure \@ref(fig:multi-far-fig) shows the FAR for each classifier per attack type. In general, `rpart` has the worst FAR for all attacks. While, `naive_bayes` and `parRF` have the best FARs especially ffor DoS and Probing attacks. The FAR of R2L and U2R attacks is low (near zero) because they do not have a lot of samples.

```{r multi-far-fig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="False Alarm Rate (FAR) of Multi-class Classfication classifiers", fig.width=7, fig.height=4}
multi_metrics %>%
  filter(multi_label != "Normal") %>%
  ggplot(aes(x = classifier, y = far)) +
  geom_bar(stat = "identity") +
  geom_label(aes(label = sprintf('%0.5f', far), fontface = ifelse(far == min_far, 2, 1)), vjust = "inward") +
  facet_grid(multi_label~.) +
  labs(x = "Classifier", y = "False Alarm Rate (FAR)")
```

Figure \@ref(fig:multi-dr-fig) shows the DR for each classifier per attack type. In general, `naive_bayes` has the worst DR for all attacks. For DoS, most of the classifiers perform almost the same with some preference for `parRF`. For Probing, `parRF` performed much better than other classifiers. For R2L and U2R, it is really difficult to achieve a good DR because of the low number of samples in the training set.

```{r multi-dr-fig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Detection Rate (DR) of Multi-class Classfication classifiers", fig.width=7, fig.height=4}
multi_metrics %>%
  filter(multi_label != "Normal") %>%
  ggplot(aes(x = classifier, y = detection_rate)) +
  geom_bar(stat = "identity") +
  geom_label(aes(label = sprintf('%0.5f', detection_rate), fontface = ifelse(detection_rate == max_dr, 2, 1)), vjust = "inward") +
  facet_grid(multi_label~.) +
  labs(x = "Classifier", y = "Detection Rate (DR)")
```

Overall, the best classifier for multi-class classification IDS is also Random Forest `parRF` as it achieved a reasonable balance between accuracy, FAR and DR across all attacks.

# Conclusion

Two IDSs systems were built using NSL-KDD dataset and six different classifiers. The first system was a binary classification IDS where the system predicts if a TCP connection is normal or attack. The best performing classifier for the first system was Random Forest with an accuracy of about 79.8%, a false alarm system of about 3.007%, and a detection rate of about 66.7%. The second system was a multi-class classification IDS where the system predicts if a TCP connection is one of (normal, DoS, Probing, R2L, or  U2R). The best performing classifier for the second system was also Random Forest with an accuracy of about 76.67%, and a reasonable false alarm rate and detection rate for DoS and Probing attacks.

\newpage

# References {-}

<div id="refs"></div>
